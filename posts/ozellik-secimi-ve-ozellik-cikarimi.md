## **Veri Biliminde Boyutluluk: Ã–zellik SeÃ§imi ve Ã–zellik Ã‡Ä±karÄ±mÄ± ArasÄ±ndaki Kritik Fark**

Veri bilimiyle ilgilenen herkesin bildiÄŸi temel bir prensip vardÄ±r: "Garbage in, garbage out" (Ã‡Ã¶p giren, Ã§Ã¶p Ã§Ä±kar). Bir makine Ã¶ÄŸrenimi modelinin performansÄ±, ona sunduÄŸumuz verinin kalitesiyle doÄŸrudan orantÄ±lÄ±dÄ±r.

Modern veri setleri, genellikle "yÃ¼ksek boyutluluk laneti" (curse of dimensionality) olarak bilinen bir zorlukla birlikte gelir: Binlerce Ã¶zellik, hesaplama maliyetini artÄ±rÄ±r ve modelin ilgisiz "gÃ¼rÃ¼ltÃ¼" Ã¼zerinde aÅŸÄ±rÄ± Ã¶ÄŸrenmesine neden olabilir.

Bu zorluklarÄ±n Ã¼stesinden gelmek iÃ§in **Ã–zellik MÃ¼hendisliÄŸi (Feature Engineering)** kritik bir rol oynar. Bu disiplinin iki temel yaklaÅŸÄ±mÄ± olan **Ã–zellik SeÃ§imi (Feature Selection)** ve **Ã–zellik Ã‡Ä±karÄ±mÄ± (Feature Extraction)**, sÄ±klÄ±kla birbirine karÄ±ÅŸtÄ±rÄ±lsa da, temelde farklÄ± felsefelere dayanÄ±r.

Bu yazÄ±da, bu iki temel tekniÄŸi, uygulamalarÄ±nÄ± ve aralarÄ±ndaki stratejik farklarÄ± inceleyeceÄŸiz.

-----

### Ã–zellik SeÃ§imi (Feature Selection) ğŸ¯

Ã–zellik SeÃ§imi, modelin hedef deÄŸiÅŸkenini aÃ§Ä±klama gÃ¼cÃ¼ en yÃ¼ksek olan **orijinal Ã¶zelliklerin bir alt kÃ¼mesini belirleme** sÃ¼recidir. Esasen, gereksiz ve ilgisiz veriyi filtreleyerek sinyali gÃ¼rÃ¼ltÃ¼den ayÄ±rmayÄ± amaÃ§lar.

Ã–rneÄŸin, bir konut fiyatÄ± regresyon modelini ele alalÄ±m. 'Metrekare', 'oda sayÄ±sÄ±' ve 'konum' gibi Ã¶zellikler hedef deÄŸiÅŸkenle yÃ¼ksek korelasyona sahipken; 'kapÄ± zili markasÄ±' gibi Ã¶zellikler muhtemelen ilgisizdir. Ã–zellik seÃ§imi, bu ilgisiz Ã¶zellikleri sistematik olarak eler.

![Ã–zellik SeÃ§imi](../images/blog-images/ozellik-secimi.png)

Bu yÃ¶ntemin birincil hedefleri:

1.  Modelin karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± azaltmak ve hesaplama verimliliÄŸini artÄ±rmak.
2.  AÅŸÄ±rÄ± Ã¶ÄŸrenme riskini minimize etmek.
3.  Modelin yorumlanabilirliÄŸini korumak veya artÄ±rmak.

Python'daki `scikit-learn` kÃ¼tÃ¼phanesi, bu iÅŸlem iÃ§in Ã§eÅŸitli mekanizmalar sunar. Ã–rneÄŸin `SelectKBest` sÄ±nÄ±fÄ±, ANOVA F-testi gibi istatistiksel yÃ¶ntemler kullanarak en yÃ¼ksek puana sahip 'k' adet Ã¶zelliÄŸi filtreler:

```python
import pandas as pd
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.datasets import load_iris

# Iris veri setini yÃ¼kleyelim
X, y = load_iris(return_X_y=True, as_frame=True)
print(f"Orijinal Ã¶zellik sayÄ±sÄ± ({X.shape[1]} adet): {list(X.columns)}")

# Ä°statistiksel olarak en iyi 2 Ã¶zelliÄŸi seÃ§meyi hedefleyelim
selector = SelectKBest(score_func=f_classif, k=2)
X_new = selector.fit_transform(X, y)

# SeÃ§ilen Ã¶zelliklerin isimlerini alalÄ±m
selected_features = selector.get_feature_names_out()
print(f"SeÃ§ilen en iyi 2 Ã¶zellik: {list(selected_features)}")
```

**Ã‡Ä±ktÄ±:**
```
Orijinal Ã¶zellik sayÄ±sÄ± (4 adet): ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
SeÃ§ilen en iyi 2 Ã¶zellik: ['petal length (cm)', 'petal width (cm)']
```

GÃ¶rÃ¼ldÃ¼ÄŸÃ¼ Ã¼zere, 4 orijinal Ã¶zellikten, hedef deÄŸiÅŸkeni (Ã§iÃ§ek tÃ¼rÃ¼) aÃ§Ä±klamada en baÅŸarÄ±lÄ± olan 2 tanesi korunmuÅŸ, diÄŸerleri elenmiÅŸtir.

-----

### Ã–zellik Ã‡Ä±karÄ±mÄ± (Feature Extraction) âœ¨

Ã–zellik Ã‡Ä±karÄ±mÄ±, mevcut Ã¶zellik uzayÄ±nÄ± **matematiksel olarak dÃ¶nÃ¼ÅŸtÃ¼rerek** daha dÃ¼ÅŸÃ¼k boyutlu yeni bir uzay yaratan bir tekniktir. Bu yeni Ã¶zellikler, orijinal verinin birer *kombinasyonudur*.

Buradaki amaÃ§, orijinal verideki varyansÄ±n veya bilginin bÃ¼yÃ¼k bir kÄ±smÄ±nÄ±, daha az sayÄ±da *yeni* Ã¶zelliÄŸe 'yoÄŸunlaÅŸtÄ±rmaktÄ±r'.

GÃ¶rÃ¼ntÃ¼ iÅŸleme buna klasik bir Ã¶rnektir. 1000x1000 piksellik bir gÃ¶rÃ¼ntÃ¼, 1 milyon Ã¶zellik anlamÄ±na gelir. Bu yÃ¼ksek boyutlulukla Ã§alÄ±ÅŸmak verimsizdir. Ã–zellik Ã§Ä±karÄ±mÄ±, bu 1 milyon pikseli, verinin yapÄ±sÄ±nÄ± temsil eden 100-200 adet "bileÅŸene" dÃ¶nÃ¼ÅŸtÃ¼rebilir.

![Ã–zellik Ã‡Ä±karÄ±mÄ±](../images/blog-images/ozellik-cikarimi.png)

En yaygÄ±n tekniklerden biri olan **PCA (Principal Component Analysis - Temel BileÅŸen Analizi)**, verideki maksimum varyansÄ± aÃ§Ä±klayan yeni, ortogonal (birbirine dik) eksenler (bileÅŸenler) bulur. AynÄ± Iris verisi Ã¼zerinde uygulayalÄ±m:

```python
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True, as_frame=True)
print(f"Orijinal Ã¶zellik sayÄ±sÄ±: {X.shape[1]}")

# Orijinal 4 Ã¶zelliÄŸi 2 yeni bileÅŸene dÃ¶nÃ¼ÅŸtÃ¼relim
pca = PCA(n_components=2)
X_new_pca = pca.fit_transform(X)

# Yeni bileÅŸenleri DataFrame olarak gÃ¶relim
pca_df = pd.DataFrame(data=X_new_pca, columns=['BileÅŸen 1', 'BileÅŸen 2'])

print(f"Yeni Ã¶zellik sayÄ±sÄ±: {pca_df.shape[1]}")
print("DÃ¶nÃ¼ÅŸÃ¼m sonrasÄ± yeni Ã¶zellikler (ilk 5 satÄ±r):")
print(pca_df.head())
```

**Ã‡Ä±ktÄ±:**

```
Orijinal Ã¶zellik sayÄ±sÄ±: 4
Yeni Ã¶zellik sayÄ±sÄ±: 2
DÃ¶nÃ¼ÅŸÃ¼m sonrasÄ± yeni Ã¶zellikler (ilk 5 satÄ±r):
   BileÅŸen 1  BileÅŸen 2
0  -2.684126   0.319397
1  -2.714142  -0.177001
2  -2.888991  -0.144949
3  -2.745343  -0.318299
4  -2.728717   0.326755
```

Dikkat edilirse, "BileÅŸen 1" ve "BileÅŸen 2" olarak adlandÄ±rÄ±lan yeni Ã¶zellikler, orijinal "Ã§anak yaprak uzunluÄŸu" gibi doÄŸrudan yorumlanabilir Ã¶zelliklerin yerini almÄ±ÅŸtÄ±r.

-----

### Temel FarklÄ±lÄ±klar ve Stratejik SeÃ§im

Ä°ki yaklaÅŸÄ±m arasÄ±ndaki farklarÄ± ve kullanÄ±m senaryolarÄ±nÄ± netleÅŸtirelim.

| Kriter | Ã–zellik SeÃ§imi (Feature Selection) | Ã–zellik Ã‡Ä±karÄ±mÄ± (Feature Extraction) |
| :--- | :--- | :--- |
| **Temel Felsefe** | Orijinal Ã¶zelliklerin bir **alt kÃ¼mesini seÃ§er**. | Orijinal Ã¶zelliklerden **yeni Ã¶zellikler tÃ¼retir**. |
| **Veri** | Ã–zellikler korunur (DÃ¶nÃ¼ÅŸÃ¼m yoktur). | Ã–zellikler dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r (Orijinaller kaybolur). |
| **Yorumlanabilirlik** | **YÃ¼ksektir.** (Hangi Ã¶zelliÄŸin Ã¶nemli olduÄŸu bilinir). | **DÃ¼ÅŸÃ¼ktÃ¼r.** (Yeni bileÅŸenlerin anlamÄ± karmaÅŸÄ±ktÄ±r). |
| **BaÅŸlÄ±ca AmaÃ§** | AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi Ã¶nlemek, yorumlanabilirliÄŸi artÄ±rmak. | YÃ¼ksek boyutluluÄŸu (Ã¶rn. \>1000 Ã¶zellik) yÃ¶netmek. |
| **PopÃ¼ler YÃ¶ntemler** | Filtre (Ã¶rn. Chi2, ANOVA), Sarma (RFE), GÃ¶mÃ¼lÃ¼ (Lasso) | PCA, LDA, t-SNE, Autoencoder'lar |

**Bir Metafor Kullanmak Gerekirse:**

  * **Ã–zellik SeÃ§imi:** 20 kitaplÄ±k bir kÃ¼tÃ¼phaneden, projenizle en ilgili 5 kitabÄ± *seÃ§ip* rafa koymaktÄ±r. Raftaki kitaplar hala orijinal kitaplardÄ±r.
  * **Ã–zellik Ã‡Ä±karÄ±mÄ±:** Bu 20 kitabÄ±n tamamÄ±nÄ± okuyup, iÃ§erdikleri ana fikirleri 3 sayfalÄ±k bir *Ã¶zet* (sentez) halinde yeniden yazmaktÄ±r. Elinizdeki artÄ±k orijinal kitaplar deÄŸil, onlarÄ±n yoÄŸunlaÅŸtÄ±rÄ±lmÄ±ÅŸ bir temsilidir.

-----

### SonuÃ§: Hangi YaklaÅŸÄ±m Ne Zaman Tercih Edilmeli?

Ã–zellik SeÃ§imi ve Ã–zellik Ã‡Ä±karÄ±mÄ± arasÄ±nda mutlak bir "kazanan" yoktur; yalnÄ±zca projenin gereksinimlerine gÃ¶re optimize edilmesi gereken **stratejik bir tercih** mevcuttur.

1.  EÄŸer projenizin Ã¶nceliÄŸi **yorumlabilirlik** ise (Ã¶rneÄŸin, tÄ±bbi bir teÅŸhis, finansal risk veya kredi skoru modellemesi gibi kararlarÄ±n "neden" alÄ±ndÄ±ÄŸÄ±nÄ±n aÃ§Ä±klanmasÄ± gereken durumlar), orijinal Ã¶zellikleri koruyan **Ã–zellik SeÃ§imi** tercih edilmelidir.
2.  EÄŸer Ã¶ncelik, yÃ¼ksek boyutlu verilerde (gÃ¶rÃ¼ntÃ¼, ses, yapÄ±landÄ±rÄ±lmamÄ±ÅŸ metin) **tahmin doÄŸruluÄŸunu maksimize etmekse** ve yorumlanabilirlik ikinci plandaysa, **Ã–zellik Ã‡Ä±karÄ±mÄ±** (PCA gibi) genellikle daha Ã¼stÃ¼n performans saÄŸlar.

Her iki teknik de, daha verimli, daha hÄ±zlÄ± ve daha doÄŸru makine Ã¶ÄŸrenimi modelleri oluÅŸturmak iÃ§in vazgeÃ§ilmez araÃ§lardÄ±r.